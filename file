#!/usr/bin/env python3
import os, time, csv, re
from multiprocessing import Pool, cpu_count

# -------- regexes (case-insensitive) --------
RE_PATH_HDR   = re.compile(r'^\s*Path\s+\d+:\s*(.*)$', re.IGNORECASE)              # capture tail of header
RE_BEGINPOINT = re.compile(r'^\s*(?:Beginpoint|Startpoint):\s*([^\s]+)', re.IGNORECASE)
RE_ENDPOINT   = re.compile(r'^\s*Endpoint:\s*([^\s]+)', re.IGNORECASE)
RE_GROUP      = re.compile(r'^\s*Path\s+Groups?:\s*\{([^}]*)\}', re.IGNORECASE)
RE_SLACK_A    = re.compile(r'^\s*Slack(?:\s+Time)?\s*([+-]?\d+(?:\.\d+)?)', re.IGNORECASE)            # "Slack Time  -0.064 | ..."
RE_SLACK_B    = re.compile(r'^\s*slack\s*\([^)]*\)\s*([+-]?\d+(?:\.\d+)?)', re.IGNORECASE)            # "slack (VIOLATED) -0.064"

RE_NEW_BLOCK  = re.compile(r'^\s*Path\s+\d+:', re.IGNORECASE)  # boundary between paths

def flush(block_rows, path_last, endpoint, beginpoint, group, slack):
    if any([path_last, endpoint, beginpoint, group, slack is not None]):
        block_rows.append([path_last or "", endpoint or "", beginpoint or "", group or "", "" if slack is None else slack])

def parse_one_file(outdir, rpt_path):
    """Parse one report, write <outdir>/<basename>_parsed.csv, return timing."""
    t0 = time.time()
    rows = []

    # per-path accumulators
    path_last = endpoint = beginpoint = group = None
    slack = None

    with open(rpt_path, "r", errors="ignore") as f:
        for line in f:
            # New path header â†’ flush previous block
            if RE_NEW_BLOCK.match(line):
                m = RE_PATH_HDR.match(line)
                if m:
                    tail = m.group(1)
                    toks = tail.split()
                    path_last = toks[-1] if toks else ""
                flush(rows, path_last, endpoint, beginpoint, group, slack)
                # start fresh for this block; keep path_last we just computed
                endpoint = beginpoint = group = None
                slack = None
                continue

            m = RE_BEGINPOINT.match(line)
            if m:
                beginpoint = m.group(1)
                continue

            m = RE_ENDPOINT.match(line)
            if m:
                endpoint = m.group(1)
                continue

            m = RE_GROUP.match(line)
            if m:
                group = m.group(1).strip()
                continue

            m = RE_SLACK_A.match(line) or RE_SLACK_B.match(line)
            if m:
                try:
                    slack = float(m.group(1))
                except ValueError:
                    slack = m.group(1)
                continue

    # flush last block (if file didn't end with a new header)
    flush(rows, path_last, endpoint, beginpoint, group, slack)

    # write per-file CSV
    os.makedirs(outdir, exist_ok=True)
    base = os.path.splitext(os.path.basename(rpt_path))[0]
    out_csv = os.path.join(outdir, f"{base}_parsed.csv")
    with open(out_csv, "w", newline="") as outf:
        w = csv.writer(outf)
        w.writerow(["path_last", "endpoint", "beginpoint", "pathgroup", "slack"])
        w.writerows(rows)

    return rpt_path, out_csv, time.time() - t0


def run(report_files, outdir="parsed_reports"):
    with Pool(processes=cpu_count()) as pool:
        for rpt_path, out_csv, secs in pool.starmap(parse_one_file, [(outdir, f) for f in report_files]):
            print(f"Processed {rpt_path} -> {out_csv} in {secs:.2f}s")

if __name__ == "__main__":
    files = [
        "/path/to/report1.rpt",
        "/path/to/report2.rpt",
        # ...
    ]
    run(files, outdir="parsed_reports")
