from pathlib import Path
import pandas as pd
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed

# --- your single-file processor ---
def your_func(path: str) -> pd.DataFrame:
    # do your processing and return a DataFrame
    df = pd.read_csv(path)  # example only
    return df

def _process_one(path: str) -> pd.DataFrame:
    df = your_func(path)
    # optional: tag where each row came from
    return df.assign(_src=path)

def parallel_concat(files, workers: int | None = None) -> pd.DataFrame:
    files = [str(p) for p in files]
    results = []
    # choose #workers explicitly or default to CPU count
    with ProcessPoolExecutor(max_workers=workers or mp.cpu_count()) as ex:
        futmap = {ex.submit(_process_one, f): f for f in files}
        for fut in as_completed(futmap):
            f = futmap[fut]
            try:
                results.append(fut.result())
            except Exception as e:
                # don't halt the whole run if one file fails
                print(f"[WARN] Failed: {f} -> {e}")

    if not results:
        return pd.DataFrame()
    return pd.concat(results, ignore_index=True, sort=False)

if __name__ == "__main__":
    # example: process all CSVs in a folder
    folder = Path("data")
    all_files = sorted(folder.glob("*.csv"))  # or any iterable of paths
    final_df = parallel_concat(all_files, workers=None)  # None -> auto CPU count
    print(final_df.shape, list(final_df.columns))